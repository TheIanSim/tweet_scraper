{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e585b1",
   "metadata": {
    "id": "59e585b1",
    "outputId": "e0c2a08a-c7b8-464d-d54c-99df42e01318"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sahilsharma/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344a578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - textblob\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/anaconda/osx-64\n",
      "  - https://conda.anaconda.org/anaconda/noarch\n",
      "  - http://conda.anaconda.org/gurobi/osx-64\n",
      "  - http://conda.anaconda.org/gurobi/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda textblob -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05e5c6",
   "metadata": {
    "id": "da05e5c6"
   },
   "source": [
    "# SNSCRAPE to scrape through Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c9221f",
   "metadata": {
    "id": "b8c9221f",
    "outputId": "051a1a34-a1ba-4a6a-f5f2-401053c2cab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToneVays\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/fdcc3fkd0fv2_1qh5sqs4ww40000gn/T/ipykernel_1725/3220756499.py:22: FutureWarning: username is deprecated, use user.username instead\n",
      "  tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officialmcafee\n",
      "\n",
      "VitalikButerin\n",
      "\n",
      "aantop\n",
      "\n",
      "TimDraper\n",
      "\n",
      "rogerkver\n",
      "\n",
      "elonmusk\n",
      "\n",
      "CathieDWood\n",
      "\n",
      "jack\n",
      "\n",
      "michael_saylor\n",
      "\n",
      "CobraBitcoin\n",
      "\n",
      "ErikVoorhees\n",
      "\n",
      "Bitboy_Crypto\n",
      "\n",
      "VinnyLingham\n",
      "\n",
      "adam3us\n",
      "\n",
      "gavinandresen\n",
      "\n",
      "NickSzabo4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Creating list to append tweet data\n",
    "tweets_list = []\n",
    "#keyword\n",
    "keyword = 'facebook'\n",
    "# No of tweets\n",
    "noOfTweet = 50000\n",
    "\n",
    "\n",
    "#Loop through the usernames:\n",
    "\n",
    "#user_names = open('News_Station.txt','r')\n",
    "user_names = open('/Users/sahilsharma/Desktop/Capstone 5102/5102_Capstone_Team/Datasets/Sentiment Analysis/Influential_People.txt','r')\n",
    "\n",
    "for user in user_names:\n",
    "    print (user)\n",
    "    \n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"from:\"+ user +\" \"+keyword ).get_items()):\n",
    "        if i > int(noOfTweet):\n",
    "            break\n",
    "        \n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.username])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec302f8",
   "metadata": {
    "id": "6ec302f8"
   },
   "source": [
    "# Creating and cleaning the dataframe from the tweets list above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ad0b9b",
   "metadata": {
    "id": "a7ad0b9b"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe from the tweets list above \n",
    "df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'],unit='ms').dt.tz_convert('Asia/Singapore')\n",
    "df['Datetime'] = df['Datetime'].apply(lambda a: datetime.datetime.strftime(a,\"%d-%m-%Y %H:%M:%S\"))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])          \n",
    "df['Tweet Id'] = ('\"'+ df['Tweet Id'].astype(str) + '\"')      \n",
    "\n",
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = re.sub('@[A-Za-z0â€“9]+', '', text) #Removing @mentions\n",
    "    text = re.sub('#', '', text) # Removing '#' hash tag\n",
    "    text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
    "    return text\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(cleanTxt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227674e3",
   "metadata": {
    "id": "227674e3"
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a088ad",
   "metadata": {
    "id": "16a088ad"
   },
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "#Iterating over the tweets in the dataframe\n",
    "\n",
    "def apply_analysis(tweet):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(tweet)\n",
    "\n",
    "\n",
    "df[['neg','neu','pos','compound']] = df['Text'].apply(apply_analysis).apply(pd.Series)\n",
    "\n",
    "def sentimental_analysis(df):\n",
    "    if df['neg'] > df['pos']:\n",
    "        return 'Negative'\n",
    "    elif df['pos'] > df['neg']:\n",
    "        return 'Positive'\n",
    "    elif df['pos'] == df['neg']:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment_NLTK'] = df.apply(sentimental_analysis, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3085006",
   "metadata": {
    "id": "b3085006"
   },
   "source": [
    "Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b15d9ef",
   "metadata": {
    "id": "3b15d9ef"
   },
   "outputs": [],
   "source": [
    "def getSubjectivity(twt):\n",
    "    return TextBlob(twt).sentiment.subjectivity\n",
    "def getPolarity(twt):\n",
    "    return TextBlob(twt).sentiment.polarity\n",
    "def getSentiment(score):\n",
    "    if score<0:\n",
    "        return 'Negative'\n",
    "    elif score==0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "    \n",
    "df['Subjectivity']=df['Text'].apply(getSubjectivity)\n",
    "df['Polarity']=df['Text'].apply(getPolarity)    \n",
    "df['Sentiment_TB']=df['Polarity'].apply(getSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1822b",
   "metadata": {
    "id": "71d1822b"
   },
   "source": [
    "# Generating csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcbc957",
   "metadata": {
    "id": "5dcbc957",
    "outputId": "6025a0c7-5276-462d-dddf-17d2b8b4215f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#df.to_csv('News_Station.csv', encoding='utf-8-sig')\n",
    "df.to_csv('/Users/sahilsharma/Desktop/Capstone 5102/5102_Capstone_Team/Datasets/Sentiment Analysis/ss2107.csv', encoding='utf-8-sig' ,index= False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbd5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Twitter Scrapping + Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
